http-server:
  addr: ":7091"

health:
  path: "/api/health"

# storages
#kafka:
#  servers: "${BOOTSTRAP_SERVERS:localhost:9092}"
#  producer:
#    options:
#      go.produce.channel.size: ${KAFKA_PRODUCE_SIZE:200000}

kafkago:
  servers: "${BOOTSTRAP_SERVERS:localhost:9092}"
  client_id: streaming
  debug_client: ${KAFKAGO_DEBUG_CLIENT:true}


#kafka.topic.initializer:
#  force: ${KAFKA_INIT_FORCE_SUCCESS:true}
#  topics:
#    - spot-container-log
#    - spot-job-log
#    - spot-metrics
#    - spot-events
#    - spot-analytics
#    - spot-trace
#    - spot-alert-record
#    - spot-notify-record
#    - spot-entity
#    - msp-tracing-metrics
#    - msp-jaeger-trace
#    - erda-spans
#    - erda-trace-metrics

redis:
  addr: "${REDIS_ADDR}"
  password: "${REDIS_PASSWORD}"
  db: ${REDIS_DB:0}
  master_name: "${REDIS_MASTER_NAME}"
  sentinels_addr: "${REDIS_SENTINELS_ADDR}"

mysql:
  host: "${MYSQL_HOST:localhost}"
  port: ${MYSQL_PORT:3306}
  username: "${MYSQL_USERNAME:root}"
  password: "${MYSQL_PASSWORD:123456}"
  database: "${MYSQL_DATABASE:dice}"

etcd:
  endpoints: "${ETCD_ENDPOINTS:http://localhost:2379}"
  tls:
    cert_file: "${ETCD_CERT_FILE:/certs/etcd-client.pem}"
    cert_key_file: "${ETCD_CERT_KEY_FILE:/certs/etcd-client-key.pem}"
    ca_file: "${ETCD_CA_FILE:/certs/etcd-ca.pem}"

etcd-election@index:
  root_path: "/erda/monitor-index-rollover-election"

etcd-election@table-loader:
  root_path: "/erda/monitor-ck-table-loader-election"

etcd-election@table-initializer:
  root_path: "/erda/monitor-ck-table-initializer-election"

etcd-mutex:
  root_path: "/erda/streaming"

clickhouse:
  _enable: ${CLICKHOUSE_ENABLE:true}
  addr: "${CLICKHOUSE_ADDR:localhost:9000}"
  username: "${CLICKHOUSE_USERNAME:default}"
  password: "${CLICKHOUSE_PASSWORD:default}"
  conn_open_strategy: "round_robin"
  conn_max_lifetime: "${CLICKHOUSE_CONN_MAX_LIFETIME:1m}"
  max_idle_conns: "${CLICKHOUSE_MAX_IDLE_CONNS:10}"
  max_open_conns: "${CLICKHOUSE_MAX_OPEN_CONNS:200}"
  dail_timeout: "${CLICKHOUSE_DAIL_TIMEOUT:1s}"


storage-retention-strategy@span:
  default_ttl: "${SPAN_TTL:168h}"

clickhouse.table.initializer@log:
  _enable: ${WRITE_LOG_TO_CLICKHOUSE_ENABLE:true}
  table_prefix: "logs"
  ttl_sync_interval: "${CLICKHOUSE_TABLE_LOG_TTL_SYNC_INTERVAL:1h}"
  cold_hot_enable: "${CLICKHOUSE_COLD_HOT_ENABLE:false}"
  default_ddl_files:
    - path: "conf/clickhouse/logs_ddl_create_db.sql"
      ignore_err: "false"
    - path: "conf/clickhouse/logs_ddl_create_default_tables.sql"
      ignore_err: "false"
  tenant_ddl_files:
    - path: "conf/clickhouse/logs_ddl_create_tenant_tables.sql"
      ignore_err: "true"

storage-retention-strategy@profile:
  default_ttl: "${PROFILE_TTL:24h}"

clickhouse.table.initializer@profile:
  _enable: ${WRITE_LOG_TO_CLICKHOUSE_ENABLE:true}
  ttl_sync_interval: "${CLICKHOUSE_TABLE_LOG_TTL_SYNC_INTERVAL:1h}"
  cold_hot_enable: "${CLICKHOUSE_COLD_HOT_ENABLE:false}"
  database: "pyroscope"
  default_ddl_files:
    - path: "conf/clickhouse/profiles_ddl_create_db.sql"
      ignore_err: "false"
    - path: "conf/clickhouse/profiles_ddl_create_default_tables.sql"
      ignore_err: "false"


clickhouse.table.creator@log:
  _enable: ${WRITE_LOG_TO_CLICKHOUSE_ENABLE:true}
  ddl_template: "conf/clickhouse/logs_ddl_create_tenant_tables.sql"
  default_write_table: "logs"
  table_prefix: "logs"

clickhouse.table.loader@log:
  _enable: ${WRITE_LOG_TO_CLICKHOUSE_ENABLE:true}
  load_mode: ${QUERY_LOG_FROM_CLICKHOUSE_LOAD_MODE:LoadFromClickhouseOnly}
  table_prefix: "logs"
  default_search_table: "logs_all"
  cache_key_prefix: "clickhouse-table-log"

clickhouse.table.loader@profile:
  _enable: ${WRITE_LOG_TO_CLICKHOUSE_ENABLE:true}
  load_mode: ${QUERY_LOG_FROM_CLICKHOUSE_LOAD_MODE:LoadFromClickhouseOnly}
  table_prefix: "main"
  default_search_table: "main"
  cache_key_prefix: "clickhouse-table-profile"

log-storage-clickhouse:
  _enable: ${WRITE_LOG_TO_CLICKHOUSE_ENABLE:true}

# elasticsearch for log
elasticsearch@log:
  _enable: ${LOG_ELASTICSEARCH_ENABLE:false}
  urls: "${LOG_ELASTICSEARCH_URL:http://localhost:9200}"
  security: ${LOG_ELASTICSEARCH_SECURITY_ENABLE:false}
  username: "${LOG_ELASTICSEARCH_SECURITY_USERNAME}"
  password: "${LOG_ELASTICSEARCH_SECURITY_PASSWORD}"

elasticsearch.index.initializer@log:
  _enable: ${WRITE_LOG_TO_ES_ENABLE:false}
  templates:
    - name: "erda-logs"
      path: "conf/log_index_template.json"

elasticsearch.index.loader@log:
  _enable: ${WRITE_LOG_TO_ES_ENABLE:false}
  load_mode: "LoadFromElasticSearchOnly"
  index_reload_interval: "1m"
  match:
    - prefix: "erda-logs-"
      patterns:
        - "<org>-{number}"
        - "<org>.<key>-{number}"

elasticsearch.index.creator@log:
  _enable: ${WRITE_LOG_TO_ES_ENABLE:false}
  patterns:
    - first_index: "erda-logs-<org>-000001"
      alias: "erda-logs-<org>-rollover"
    - first_index: "erda-logs-<org>.<key>-000001"
      alias: "erda-logs-<org>.<key>-rollover"
  remove_conflicting_indices: true

elasticsearch.index.rollover@log:
  _enable: ${WRITE_LOG_TO_ES_ENABLE:false}
  check_interval: "30s"
  body_file: "conf/index_rollover.json"
  patterns:
    - index: "erda-logs-<org>-{number}"
      alias: "erda-logs-<org>-rollover"
    - index: "erda-logs-<org>.<key>-{number}"
      alias: "erda-logs-<org>.<key>-rollover"

storage-retention-strategy@log:
  load_from_database: true
  ttl_reload_interval: "3m"
  default_ttl: "${LOG_TTL:168h}"

log-storage-elasticsearch:
  _enable: ${WRITE_LOG_TO_ES_ENABLE:false}
  write_timeout: "1m"
  index_type: "logs"

log-persist:
  _enable: ${WRITE_LOG_TO_ES_ENABLE|WRITE_LOG_TO_CLICKHOUSE_ENABLE:true}
  input:
    topics: "${LOG_TOPICS:spot-container-log,spot-job-log}"
    group: "${LOG_GROUP_ID:erda-logs-dev}"
    offsets:
      initial: earliest
  id_keys: "${LOG_ID_KEYS:TERMINUS_DEFINE_TAG,terminus_define_tag,MESOS_TASK_ID,mesos_task_id}"
  read_timeout: "5s"
  buffer_size: ${LOG_BATCH_SIZE:4096}
  parallelism: ${LOG_PERSIST_PARALLELISM:6}
  storage_writer_service: "${LOG_STORAGE_WRITER_SERVICE:log-storage-clickhouse-writer}"
  print_invalid_log: false

cassandra:
  _enable: ${CASSANDRA_ENABLE:false}
  host: "${CASSANDRA_ADDR:localhost:9042}"
  security: ${CASSANDRA_SECURITY_ENABLE:false}
  username: ${CASSANDRA_SECURITY_USERNAME}
  password: ${CASSANDRA_SECURITY_PASSWORD}
  timeout: "${CASSANDRA_TIMEOUT:3s}"

grpc-client@erda.core.org:
  _enable: ${WRITE_LOG_TO_CASSANDRA_ENABLE:false}
  addr: "${ERDA_SERVER_GRPC_ADDR:erda-server:8096}"
erda.core.org-client:
  _enable: ${WRITE_LOG_TO_CASSANDRA_ENABLE:false}
erda.core.org:
  _enable: ${WRITE_LOG_TO_CASSANDRA_ENABLE:false}

log-persist-v1:
  _enable: ${WRITE_LOG_TO_CASSANDRA_ENABLE:false}
  input:
    topics: "${LOG_TOPICS:spot-container-log,spot-job-log}"
    group: "${LOG_GROUP_ID:spot-monitor-log-dev}"
    parallelism: ${LOG_CONSUMERS:3}
    offsets:
      initial: earliest
  output:
    id_keys: "${LOG_ID_KEYS:TERMINUS_DEFINE_TAG,terminus_define_tag,MESOS_TASK_ID,mesos_task_id}"
    log_schema:
      org_refresh_interval: "${LOG_SCHEMA_ORG_REFRESH_INTERVAL:3m}"
    cassandra:
      writer_config:
        parallelism: ${CASSANDRA_PARALLELISM:3}
        batch:
          size_bytse: ${CASSANDRA_BATCH_SIZE_BYTES:307200}
          size: ${CASSANDRA_BATCH_SIZE:50}
          timeout: ${CASSANDRA_BATCH_TIMEOUT:10s}
        retry: ${CASSANDRA_WRITE_RETRY:-1}  # -1 means block forever. kafka will handle the issue of stream block
      session_config:
        keyspace:
          name: "spot_prod"
          auto: ${LOG_STORE_CASSANDRA_SESSION_KEYSPACE_AUTO:true} # 自动创建 keyspace
          replication:
            class: ${CASSANDRA_KEYSPACE_REPLICATION_CLASS:SimpleStrategy}
            factor: ${CASSANDRA_KEYSPACE_REPLICATION_FACTOR:2}
        reconnection:
          check_interval: ${LOG_STORE_CASSANDRA_RECONNECTION_CHECK_INTERVAL:3m}
      gc_grace_seconds: 86400

# elasticsearch for event
elasticsearch@event:
  _enable: ${EVENT_ELASTICSEARCH_ENABLE:false}
  urls: "${EVENT_ELASTICSEARCH_URL:http://localhost:9200}"
  security: ${EVENT_ELASTICSEARCH_SECURITY_ENABLE:false}
  username: "${EVENT_ELASTICSEARCH_SECURITY_USERNAME}"
  password: "${EVENT_ELASTICSEARCH_SECURITY_PASSWORD}"

elasticsearch.index.initializer@event:
  _enable: ${WRITE_EVENT_TO_ES_ENABLE:false}
  templates:
    - name: "erda-events"
      path: "conf/event_index_template.json"

elasticsearch.index.loader@event:
  _enable: ${WRITE_EVENT_TO_ES_ENABLE:false}
  load_mode: "LoadFromElasticSearchOnly"
  index_reload_interval: "1m"
  match:
    - prefix: "erda-events-"
      patterns:
        - "<event>-<namespace>-{number}"
        - "<event>-<namespace>.<key>-{number}"

elasticsearch.index.creator@event:
  _enable: ${WRITE_EVENT_TO_ES_ENABLE:false}
  patterns:
    - first_index: "erda-events-<event>-<namespace>-000001"
      alias: "erda-events-<event>-<namespace>-rollover"
    - first_index: "erda-events-<event>-<namespace>.<key>-000001"
      alias: "erda-events-<event>-<namespace>.<key>-rollover"
  remove_conflicting_indices: true

elasticsearch.index.rollover@event:
  _enable: ${WRITE_EVENT_TO_ES_ENABLE:false}
  check_interval: "30s"
  body_file: "conf/index_rollover.json"
  patterns:
    - index: "erda-events-<event>-<namespace>-{number}"
      alias: "erda-events-<event>-<namespace>-rollover"
    - index: "erda-events-<event>-<namespace>.<key>-{number}"
      alias: "erda-events-<event>-<namespace>.<key>-rollover"

storage-retention-strategy@event:
  _enable: ${WRITE_EVENT_TO_ES_ENABLE|WRITE_EVENT_TO_CLICKHOUSE_ENABLE:true}
  load_from_database: false
  ttl_reload_interval: "3m"

event-storage-elasticsearch:
  _enable: ${WRITE_EVENT_TO_ES_ENABLE:false}
  write_timeout: "1m"
  index_type: "events"

event-persist:
  _enable: ${WRITE_EVENT_TO_ES_ENABLE|WRITE_EVENT_TO_CLICKHOUSE_ENABLE:true}
  input:
    topics: "${EVENT_TOPICS:spot-events}"
    group: "${EVENT_GROUP_ID:spot-monitor-event-dev}"
  read_timeout: "5s"
  buffer_size: ${EVENT_BATCH_SIZE:200}
  parallelism: ${EVENT_PERSIST_PARALLELISM:3}
  print_invalid_event: false
  storage_writer_service: ${EVENT_STORAGE_WRITER_SERVICE:event-storage-clickhouse-writer}

# elasticsearch for metric
elasticsearch:
  _enable: ${ES_ENABLE:false}
  urls: "${ES_URL:http://localhost:9200}"
  security: ${ES_SECURITY_ENABLE:false}
  username: "${ES_SECURITY_USERNAME}"
  password: "${ES_SECURITY_PASSWORD}"

elasticsearch.index.initializer@metric:
  _enable: ${WRITE_METRIC_TO_ES_ENABLE:false}
  templates:
    - name: "spot_metric_template"
      path: "conf/metric_index_template.json"

elasticsearch.index.loader@metric:
  _enable: ${WRITE_METRIC_TO_ES_ENABLE:false}
  load_mode: "LoadFromElasticSearchOnly"
  index_reload_interval: "1m"
  match:
    - prefix: "spot-"
      patterns:
        - "empty"
        - "<metric>-<namespace>.<key>-r-{number}"
        - "<metric>-<namespace>-r-{number}"
        - "<metric>-<namespace>-m-{}"
        - "<metric>-<namespace>-m-{}"
        - "<metric>-<namespace>.<key>-<timestamp>"
        - "<metric>-<namespace>-<timestamp>"
        - "<metric>-<namespace>.<key>"
        - "<metric>-<namespace>"

elasticsearch.index.creator@metric:
  _enable: ${WRITE_METRIC_TO_ES_ENABLE:false}
  patterns:
    - first_index: "spot-<metric>-<namespace>-r-000001"
      alias: "spot-<metric>-<namespace>-rollover"
    - first_index: "spot-<metric>-<namespace>.<key>-r-000001"
      alias: "spot-<metric>-<namespace>.<key>-rollover"
  fixed_patterns:
    - "spot-<metric>-<namespace>"
    - "spot-<metric>-<namespace>.<key>"
  remove_conflicting_indices: true

elasticsearch.index.rollover@metric:
  _enable: ${WRITE_METRIC_TO_ES_ENABLE:false}
  check_interval: "30m"
  body_file: "conf/index_rollover.json"
  patterns:
    - index: "spot-<metric>-<namespace>-r-{number}"
      alias: "spot-<metric>-<namespace>-rollover"
    - index: "spot-<metric>-<namespace>.<key>-r-{number}"
      alias: "spot-<metric>-<namespace>.<key>-rollover"

storage-retention-strategy@metric:
  load_from_database: true
  ttl_reload_interval: "3m"

metric-storage-elasticsearch:
  _enable: ${WRITE_METRIC_TO_ES_ENABLE:false}
  read_timeout: "1m"
  write_timeout: "1m"
  index_type: "spot"
  dummy_index: "spot-empty"

metric-persist:
  _enable: ${WRITE_METRIC_TO_ES_ENABLE:false}
  input:
    topics: "${METRIC_TOPICS:spot-metrics,erda-trace-metrics}"
    group: "${METRIC_GROUP_ID:spot-monitor-metrics-dev}"
  read_timeout: ${METRIC_PERSIST_READ_TIMEOUT:10s}
  buffer_size: ${METRIC_BATCH_SIZE:200}
  parallelism: ${METRIC_PERSIST_PARALLELISM:3}
  features:
    generate_meta: true
    machine_summary: true

# elasticsearch for span
elasticsearch@span:
  _enable: ${SPAN_ELASTICSEARCH_ENABLE:false}
  urls: "${SPAN_ELASTICSEARCH_URL:http://localhost:9200}"
  security: ${SPAN_ELASTICSEARCH_SECURITY_ENABLE:false}
  username: "${SPAN_ELASTICSEARCH_SECURITY_USERNAME}"
  password: "${SPAN_ELASTICSEARCH_SECURITY_PASSWORD}"

elasticsearch.index.initializer@span:
  _enable: ${WRITE_SPAN_TO_ES_ENABLE:false}
  templates:
    - name: "erda-spans"
      path: "conf/span_index_template.json"

elasticsearch.index.loader@span:
  _enable: ${WRITE_SPAN_TO_ES_ENABLE:false}
  load_mode: "LoadFromElasticSearchOnly"
  index_reload_interval: "1m"
  match:
    - prefix: "erda-spans-"
      patterns:
        - "<org>-{number}"
        - "<org>.<key>-{number}"

elasticsearch.index.creator@span:
  _enable: ${WRITE_SPAN_TO_ES_ENABLE:false}
  patterns:
    - first_index: "erda-spans-<org>-000001"
      alias: "erda-spans-<org>-rollover"
    - first_index: "erda-spans-<org>.<key>-000001"
      alias: "erda-spans-<org>.<key>-rollover"
  remove_conflicting_indices: true

elasticsearch.index.rollover@span:
  _enable: ${WRITE_SPAN_TO_ES_ENABLE:false}
  check_interval: "30s"
  body_file: "conf/index_rollover.json"
  patterns:
    - index: "erda-spans-<org>-{number}"
      alias: "erda-spans-<org>-rollover"
    - index: "erda-spans-<org>.<key>-{number}"
      alias: "erda-spans-<org>.<key>-rollover"


span-storage-elasticsearch:
  _enable: ${WRITE_SPAN_TO_ES_ENABLE:false}
  write_timeout: "1m"
  index_type: "spans"

span-persist:
  _enable: ${WRITE_SPAN_TO_ES_ENABLE:false}
  spot_input:
    topics: "${SPOT_TRACE_TOPICS:spot-trace}"
    group: "${TRACE_GROUP_ID:spot-monitor-trace-dev}"
    parallelism: ${SPOT_SPOTSPAN_CONSUMERS:3}
  oap_input:
    topics: "${OAP_TRACE_TOPICS:msp-jaeger-trace,erda-spans}"
    group: "${TRACE_GROUP_ID:spot-monitor-trace-dev}"
    parallelism: ${SPOT_OAPSPAN_CONSUMERS:3}
  id_keys: "${SPAN_ID_KEYS:TERMINUS_DEFINE_TAG,terminus_define_tag,MESOS_TASK_ID,mesos_task_id}"
  read_timeout: "5s"
  buffer_size: ${SPAN_BATCH_SIZE:200}
  parallelism: ${SPAN_PERSIST_PARALLELISM:3}
  print_invalid_span: false

browser-analytics:
  _enable: ${BROWSER_ENABLE:true}
  input:
    topics: "${BROWSER_TOPICS:spot-analytics}"
    group: "${BROWSER_GROUP_ID:spot-monitor-browser-dev}"
    parallelism: ${BROWSER_CONSUMERS:3}
  output:
    topic: "${METRIC_TOPICS:spot-metrics}"
    parallelism: ${KAFKA_PARALLELISM:3}
    batch:
      size:  ${KAFKA_BATCH_SIZE:50}
      timeout: "10s"
  ipdb: "conf/ipdata.dat"

trace-storage:
  _enable: ${WRITE_SPAN_TO_CASSANDRA_ENABLE:false}
  spot_input:
    topics: "${SPOT_TRACE_TOPICS:spot-trace}"
    group: "${TRACE_GROUP_ID:spot-monitor-trace-dev}"
    parallelism: ${SPOT_TRACE_CONSUMERS:3}
  oap_input:
    topics: "${OAP_TRACE_TOPICS:erda-spans,msp-jaeger-trace}"
    group: "${TRACE_GROUP_ID:spot-monitor-trace-dev}"
    parallelism: ${OAP_TRACE_CONSUMERS:3}
  output:
    cassandra:
      writer_config:
        parallelism: ${CASSANDRA_PARALLELISM:3}
        batch:
          size: ${CASSANDRA_BATCH_SIZE:50}
          timeout: ${TRACE_STORE_CASSANDRA_BATCH_TIMEOUT:10s}
        retry: 2
      session_config:
        keyspace:
          name: "spot_prod"
          auto: true # 自动创建 keyspace
          replication:
            class: ${CASSANDRA_KEYSPACE_REPLICATION_CLASS:SimpleStrategy}
            factor: ${CASSANDRA_KEYSPACE_REPLICATION_FACTOR:2}
      gc_grace_seconds: 86400
      ttl: ${TRACE_TTL:168h}
    kafka:
      topic: "${SPOT_TRACE_TOPICS:spot-trace}"
      parallelism: ${KAFKA_PARALLELISM:3}
      batch:
        size:  ${KAFKA_BATCH_SIZE:50}
        timeout: "10s"

# entity
elasticsearch@entity:
  _enable: ${ENTITY_ELASTICSEARCH_ENABLE:false}
  urls: "${ENTITY_ELASTICSEARCH_URL:http://localhost:9200}"
  security: ${ENTITY_ELASTICSEARCH_SECURITY_ENABLE:false}
  username: "${ENTITY_ELASTICSEARCH_SECURITY_USERNAME}"
  password: "${ENTITY_ELASTICSEARCH_SECURITY_PASSWORD}"
elasticsearch.index.initializer@entity:
  _enable: ${WRITE_ENTITY_TO_ES_ENABLE:false}
  templates:
    - name: "erda-entity"
      path: "conf/entity_index_template.json"
entity-storage-elasticsearch:
  _enable: ${WRITE_ENTITY_TO_ES_ENABLE:false}
  write_timeout: "1m"
  query_timeout: "1m"
  index_type: "entity"
  pattern: "erda-entity-<type>"
entity-persist:
  input:
    topics: "${ENTITY_TOPICS:spot-entity}"
    group: "${ENTITY_GROUP_ID:spot-monitor-entity-dev}"
  read_timeout: "5s"
  buffer_size: ${ENTITY_BATCH_SIZE:200}
  parallelism: ${ENTITY_PERSIST_PARALLELISM:3}
  storage_writer_service: "${ENTITY_STORAGE_WRITER_SERVICE:entity-storage-clickhouse-writer}"

alert-event-storage:
  input:
    topics: "${ALERT_EVENT_TOPICS:spot-alert-record}"
    group: "${ALERT_EVENT_GROUP_ID:spot-alert-event-dev}"
    parallelism: ${TRACE_CONSUMERS:3}
    options:
      auto.offset.reset: "${KAFKA_AUTO_OFFSET_RESET:latest}"
      auto.commit.interval.ms: "${KAFKA_AUTO_COMMIT_INTERVAL_MS:1000}"

alert-storage:
  input:
    topics: "${TRACE_TOPICS:spot-alert-record}"
    group: "${EVENT_GROUP_ID:spot-alert-record-dev}"
    parallelism: ${TRACE_CONSUMERS:3}
    options:
      auto.offset.reset: "${KAFKA_AUTO_OFFSET_RESET:latest}"
      auto.commit.interval.ms: "${KAFKA_AUTO_COMMIT_INTERVAL_MS:1000}"

notify-storage:
  input:
    topics: "${TRACE_TOPICS:spot-notify-record}"
    group: "${EVENT_GROUP_ID:spot-notify-record-dev}"
    parallelism: ${TRACE_CONSUMERS:3}
    options:
      auto.offset.reset: "${KAFKA_AUTO_OFFSET_RESET:latest}"
      auto.commit.interval.ms: "${KAFKA_AUTO_COMMIT_INTERVAL_MS:1000}"

# --- data pipeline block ---
erda.oap.collector.core:
  pipelines:
    spans:
      - receivers:
          - "erda.oap.collector.receiver.kafka@spotspan"
          - "erda.oap.collector.receiver.kafka@oapspan"
        processors:
          - "erda.oap.collector.processor.modifier@span"
        exporters:
          - "erda.oap.collector.exporter.clickhouse@span"
        batch_size: ${EXPORTER_CLICKHOUSE_SPAN_BATCH_SIZE:81920}
        flush_interval: ${EXPORTER_CLICKHOUSE_SPAN_FLUSH_INTERVAL}
        flush_jitter: ${EXPORTER_CLICKHOUSE_SPAN_FLUSH_JITTER}
        _enable: ${WRITE_SPAN_TO_CLICKHOUSE_ENABLE:true}

    metrics:
      - receivers:
          - "erda.oap.collector.receiver.kafka@spotmetric"
          - "erda.oap.collector.receiver.kafka@oapspanevent"
        exporters:
          - "erda.oap.collector.exporter.clickhouse@metric"
        batch_size: ${EXPORTER_CLICKHOUSE_METRIC_BATCH_SIZE:81920}
        flush_interval: ${EXPORTER_CLICKHOUSE_METRIC_FLUSH_INTERVAL}
        flush_jitter: ${EXPORTER_CLICKHOUSE_METRIC_FLUSH_JITTER}
        _enable: ${WRITE_METRIC_TO_CLICKHOUSE_ENABLE:true}

    profiles:
      - receivers:
          - "erda.oap.collector.receiver.kafka@spotprofile"
        exporters:
          - "erda.oap.collector.exporter.pyroscope@profile"
        batch_size: ${EXPORTER_CLICKHOUSE_METRIC_BATCH_SIZE:4096}
        flush_interval: ${EXPORTER_CLICKHOUSE_METRIC_FLUSH_INTERVAL}
        flush_jitter: ${EXPORTER_CLICKHOUSE_METRIC_FLUSH_JITTER}
        _enable: ${WRITE_METRIC_TO_CLICKHOUSE_ENABLE:true}
# --- data pipeline block ---

# --- span to clickhouse block ---
erda.oap.collector.receiver.kafka@spotspan:
  _enable: ${WRITE_SPAN_TO_CLICKHOUSE_ENABLE:true}
  proto_parser: spotspan
  consumer:
    topics: "${SPOT_TRACE_TOPICS:spot-trace}"
    group: "${TRACE_GROUP_ID:spot-monitor-trace-dev}"

erda.oap.collector.receiver.kafka@oapspan:
  _enable: ${WRITE_SPAN_TO_CLICKHOUSE_ENABLE:true}
  proto_parser: oapspan
  consumer:
    topics: "${OAP_TRACE_TOPICS:msp-jaeger-trace,erda-spans}"
    group: "${TRACE_GROUP_ID:spot-monitor-trace-dev}"

erda.oap.collector.receiver.kafka@spotprofile:
  _enable: ${WRITE_METRIC_TO_CLICKHOUSE_ENABLE:true}
  proto_parser: spotprofile
  consumer:
    topics: "${METRIC_TOPICS:spot-profiles}"
    group: "${PROFILE_CH_GROUP_ID:spot-monitor-profiles-ch}"
    offsets:
      initial: earliest

erda.oap.collector.processor.modifier@span:
  _enable: ${WRITE_SPAN_TO_CLICKHOUSE_ENABLE:true}
  rules:
    - action: drop
      key: tags.http_read_bytes
    - action: drop
      key: tags.http_request_content_length
    - action: drop
      key: tags.http_wrote_bytes

erda.oap.collector.exporter.clickhouse@span:
  _enable: ${WRITE_SPAN_TO_CLICKHOUSE_ENABLE:true}
  storage:
    currency_num: ${EXPORTER_CH_SPAN_CURRENCY_NUM:2}
  builder:
    data_type: span
    tenant_id_key: terminus_key

clickhouse.table.initializer@span:
  _enable: ${WRITE_SPAN_TO_CLICKHOUSE_ENABLE:true}
  table_prefix: "spans"
  ttl_sync_interval: "${CLICKHOUSE_TABLE_TTL_SYNC_INTERVAL:24h}"
  cold_hot_enable: "${CLICKHOUSE_COLD_HOT_ENABLE:false}"
  default_ddl_files:
    - path: "conf/clickhouse/spans/ddl_create_db.sql.tpl"
      ignore_err: "false"
    - path: "conf/clickhouse/spans/ddl_create_default_tables.sql.tpl"
      ignore_err: "false"

clickhouse.table.loader@span:
  _enable: ${WRITE_SPAN_TO_CLICKHOUSE_ENABLE:true}
  load_mode: "${QUERY_METRIC_FROM_CLICKHOUSE_LOAD_MODE:LoadFromClickhouseOnly}"
  table_prefix: "spans"
  default_search_table: "spans_all"
  cache_key_prefix: "clickhouse-table-span"

clickhouse.table.creator@span:
  _enable: ${WRITE_SPAN_TO_CLICKHOUSE_ENABLE:true}
  default_write_table: "spans"
  table_prefix: "spans"

# --- span to clickhouse block ---

# --- metric to clickhouse block ---
erda.oap.collector.receiver.kafka@spotmetric:
  _enable: ${WRITE_METRIC_TO_CLICKHOUSE_ENABLE:true}
  proto_parser: spotmetric
  future_discard_date: "${METRIC_FUTURE_DISCARD_DATE:24h}"
  consumer:
    topics: "${METRIC_TOPICS:spot-metrics}"
    # TODO. METRIC_CH_GROUP_ID to METRIC_GROUP_ID when all finished
    group: "${METRIC_CH_GROUP_ID:spot-monitor-metrics-ch}"

erda.oap.collector.receiver.kafka@oapspanevent:
  _enable: ${WRITE_METRIC_TO_CLICKHOUSE_ENABLE:true}
  proto_parser: oapspanevent
  consumer:
    topics: "${OAP_TRACE_EVENT_TOPICS:erda-spans}"
    group: "${OAP_TRACE_EVENT_GROUP_ID:spot-monitor-trace-event}"

erda.oap.collector.exporter.clickhouse@metric:
  _enable: ${WRITE_METRIC_TO_CLICKHOUSE_ENABLE:true}
  storage:
    currency_num: ${EXPORTER_CH_METRIC_CURRENCY_NUM:2}
  builder:
    data_type: metric
    tenant_id_key: _metric_scope_id

erda.oap.collector.exporter.pyroscope@profile: {}

clickhouse.table.initializer@metric:
  _enable: ${WRITE_METRIC_TO_CLICKHOUSE_ENABLE:true}
  table_prefix: "metrics"
  ttl_sync_interval: "${CLICKHOUSE_TABLE_METRIC_TTL_SYNC_INTERVAL:1h}"
  cold_hot_enable: "${CLICKHOUSE_COLD_HOT_ENABLE:false}"
  default_ddl_files:
    - path: "conf/clickhouse/metrics/ddl_create_db.sql.tpl"
      ignore_err: "false"
    - path: "conf/clickhouse/metrics/ddl_create_default_tables.sql.tpl"
      ignore_err: "false"
  tenant_ddl_files:
    - path: "conf/clickhouse/metrics/ddl_create_tenant_tables.sql.tpl"
      ignore_err: "true"

clickhouse.table.creator@metric:
  _enable: ${WRITE_METRIC_TO_CLICKHOUSE_ENABLE:true}
  ddl_template: "conf/clickhouse/metrics/ddl_create_tenant_tables.sql.tpl"
  default_write_table: "metrics"
  table_prefix: "metrics"

clickhouse.table.loader@metric:
  _enable: ${WRITE_METRIC_TO_CLICKHOUSE_ENABLE:true}
  load_mode: "${QUERY_METRIC_FROM_CLICKHOUSE_LOAD_MODE:LoadFromClickhouseOnly}"
  table_prefix: "metrics"
  default_search_table: "metrics_all"
  cache_key_prefix: "clickhouse-table-metric"
# --- metric to clickhouse block ---

# --- event to clickhouse block ---
clickhouse.table.initializer@event:
  _enable: ${WRITE_EVENT_TO_CLICKHOUSE_ENABLE:true}
  table_prefix: "events"
  ttl_sync_interval: "${CLICKHOUSE_TABLE_TTL_SYNC_INTERVAL:24h}"
  cold_hot_enable: "${CLICKHOUSE_COLD_HOT_ENABLE:false}"
  default_ddl_files:
    - path: "conf/clickhouse/events_ddl_create_db.sql"
      ignore_err: "false"
    - path: "conf/clickhouse/events_ddl_create_default_tables.sql"
      ignore_err: "false"
  tenant_ddl_files:
    - path: "conf/clickhouse/events_ddl_create_tenant_tables.sql"
      ignore_err: "true"

clickhouse.table.creator@event:
  _enable: ${WRITE_EVENT_TO_CLICKHOUSE_ENABLE:true}
  ddl_template: "conf/clickhouse/events_ddl_create_tenant_tables.sql"
  default_write_table: "events"
  table_prefix: "events"

clickhouse.table.loader@event:
  _enable: ${WRITE_EVENT_TO_CLICKHOUSE_ENABLE:true}
  load_mode: ${QUERY_EVENT_FROM_CLICKHOUSE_LOAD_MODE:LoadFromClickhouseOnly}
  table_prefix: "events"
  default_search_table: "events_all"
  cache_key_prefix: "clickhouse-table-event"

gorm.v2:
  host: "${MYSQL_HOST}"
  port: "${MYSQL_PORT}"
  username: "${MYSQL_USERNAME}"
  password: "${MYSQL_PASSWORD}"
  database: "${MYSQL_DATABASE}"

event-storage-clickhouse:
  _enable: ${WRITE_EVENT_TO_CLICKHOUSE_ENABLE:true}
  query_timeout: 1m

# --- event to clickhouse block ---

# --- entity to clickhouse block ---
clickhouse.table.initializer@entity:
  _enable: ${WRITE_ENTITY_TO_CLICKHOUSE_ENABLE:true}
  table_prefix: "entities"
  ttl_sync_interval: "${CLICKHOUSE_TABLE_TTL_SYNC_INTERVAL:24h}"
  cold_hot_enable: "${CLICKHOUSE_COLD_HOT_ENABLE:false}"
  default_ddl_files:
    - path: "conf/clickhouse/entities_ddl_create_db.sql"
      ignore_err: "false"
    - path: "conf/clickhouse/entities_ddl_create_default_tables.sql"
      ignore_err: "false"
  tenant_ddl_files:
    - path: "conf/clickhouse/entities_ddl_create_tenant_tables.sql"
      ignore_err: "true"

clickhouse.table.creator@entity:
  _enable: ${WRITE_ENTITY_TO_CLICKHOUSE_ENABLE:true}
  ddl_template: "conf/clickhouse/entities_ddl_create_tenant_tables.sql"
  default_write_table: "entities"
  table_prefix: "entities"

clickhouse.table.loader@entity:
  _enable: ${WRITE_ENTITY_TO_CLICKHOUSE_ENABLE:true}
  load_mode: ${QUERY_ENTITY_FROM_CLICKHOUSE_LOAD_MODE:LoadFromClickhouseOnly}
  table_prefix: "entities"
  default_search_table: "entities_all"
  cache_key_prefix: "clickhouse-table-entity"
storage-retention-strategy@entity:
  _enable: ${WRITE_ENTITY_TO_ES_ENABLE|WRITE_ENTITY_TO_CLICKHOUSE_ENABLE:true}
  load_from_database: false
  ttl_reload_interval: "3m"

entity-storage-clickhouse:
  _enable: ${WRITE_ENTITY_TO_CLICKHOUSE_ENABLE:true}
  query_timeout: 1m

# --- entity to clickhouse block ---

i18n@metric:

http-server@admin:
  addr: ":7098"
pprof:
prometheus:
