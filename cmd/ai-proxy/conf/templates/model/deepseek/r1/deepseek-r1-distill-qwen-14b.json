{
  "deepseek-r1-distill-qwen-14b": {
    "placeholders": [
      {
        "name": "target-model-name",
        "type": "string",
        "required": false,
        "default": "${@template.name}"
      }
    ],
    "metadata": {
      "recommended_service_provider_templates": [
        "aliyun-bailian"
      ],
      "website": "https://bailian.console.aliyun.com/cn-beijing/?tab=model#/model-market/detail/deepseek-r1-distill-qwen-14b"
    },
    "desc": "DeepSeek R1 Distill Qwen 14B is a distilled large language model based on [Qwen 2.5 14B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B), using outputs from [DeepSeek R1](/deepseek/deepseek-r1). It outperforms OpenAI's o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\\n\\nOther benchmark results include:\\n\\n- AIME 2024 pass@1: 69.7\\n- MATH-500 pass@1: 93.9\\n- CodeForces Rating: 1481\\n\\nThe model leverages fine-tuning from DeepSeek R1's outputs, enabling competitive performance comparable to larger frontier models.",
    "config": {
      "desc": "${@template.desc}",
      "apiKey": "",
      "type": "text_generation",
      "publisher": "deepseek",
      "metadata": {
        "public": {
          "abilities": {
            "abilities": [],
            "input_modalities": [
              "text"
            ],
            "output_modalities": [
              "text"
            ]
          },
          "context": {
            "context_length": 32768,
            "max_completion_tokens": 16384,
            "max_prompt_tokens": 32768
          },
          "model_name": "${@template.placeholders.target-model-name}",
          "pricing": {
            "completion": "0.000003",
            "image": "0",
            "input_cache_read": "0",
            "input_cache_write": "0",
            "internal_reasoning": "0",
            "prompt": "0.000001",
            "request": "0",
            "unit": "CNY",
            "web_search": "0"
          }
        },
        "secret": {}
      }
    }
  }
}
